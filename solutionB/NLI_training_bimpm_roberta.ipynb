{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BpyhNiW5GerH"
      },
      "source": [
        "## **Solution B:** Deep learning-based approaches that do not employ transformer architectures  \n",
        "\n",
        "<!-- Move to README\n",
        "-START-\n",
        "**Task A:** Natural Language Inference (NLI)  \n",
        "\n",
        "*Given a premise and a hypothesis, determine if the hypothesis is true based on the premise. You will be given more than 26K premise-hypothesis pairs as training data, and more than 6K pairs as validation data.*\n",
        "\n",
        "---\n",
        "-END- -->\n",
        "\n",
        "\n",
        "Our final model uses a BiMPM-inspired architecture with frozen RoBERTa embeddings. The model captures matching perspectives between the encoded premise and hypothesis via a custom multi-perspective matching layer. Pre-trained RoBERTa embeddings are computed once and used as static inputs. The model was optimized using Optuna, and trained on the full dataset.\n",
        "\n",
        "---\n",
        "\n",
        "<!-- **Group 33:** Joudi Saeidan & Ghayadah Alsaadi   -->\n",
        "\n",
        "<!-- --- -->\n",
        "\n",
        "### **Notebook Overview**\n",
        "\n",
        "This notebook:\n",
        "- Tunes hyperparameters using Optuna  \n",
        "- Trains the final BiMPM model with best parameters  \n",
        "- Saves the model to:  \n",
        "  `/savedModels/best_bimpm_model.keras`  \n",
        "- Evaluates the model on the dev set  \n",
        "<!-- - Saves predictions to `.predict` and `.zip` files for submission   -->\n",
        "\n",
        ">  *Demo code for loading and using this model is provided in a separate notebook.*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WWXptIznjPKt"
      },
      "source": [
        " Setup and Install Packages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XVsDzaRyjVRn",
        "outputId": "db42d407-bf89-4471-e906-a59c1e8b1c8c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive')\n",
        "\n",
        "!pip install transformers tensorflow scikit-learn optuna --quiet\n",
        "\n",
        "import os, re, string, zipfile, gc\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Input, LSTM, Bidirectional, Dense, Dropout, Concatenate, Lambda, GlobalAveragePooling1D, GlobalMaxPooling1D, Layer\n",
        "from transformers import AutoTokenizer, TFAutoModel\n",
        "from sklearn.metrics import classification_report\n",
        "import optuna\n",
        "from keras.saving import register_keras_serializable\n",
        "\n",
        "\n",
        "# model_path = \"/content/drive/MyDrive/dataset/training_data/NLI/best_bimpm_model.keras\"\n",
        "# train_path = \"/content/drive/MyDrive/dataset/training_data/NLI/train.csv\"\n",
        "# dev_path   = \"/content/drive/MyDrive/dataset/training_data/NLI/dev.csv\"\n",
        "\n",
        "model_path = \"savedModels/best_bimpm_model.keras\"\n",
        "train_path = \"../Data/train.csv\"\n",
        "dev_path   = \"../Data/dev.csv\"\n",
        "\n",
        "MODEL_NAME = \"roberta-base\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uRVGm79Gjeku"
      },
      "source": [
        "Load and Preprocess Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "YqkvHXNkjgG-"
      },
      "outputs": [],
      "source": [
        "def clean_text(text):\n",
        "    text = ''.join(ch for ch in text if ch not in string.punctuation)\n",
        "    text = re.sub(r'\\s+', ' ', text).strip()\n",
        "    return text\n",
        "\n",
        "def load_and_preprocess_data(train_path,dev_path):\n",
        "    train_df = pd.read_csv(train_path)\n",
        "    dev_df   = pd.read_csv(dev_path)\n",
        "\n",
        "    for df in (train_df, dev_df):\n",
        "        df['premise'] = df['premise'].fillna(\"\").apply(clean_text)\n",
        "        df['hypothesis'] = df['hypothesis'].fillna(\"\").apply(clean_text)\n",
        "\n",
        "    train_labels = train_df.label.values.astype(\"int32\")\n",
        "    dev_labels   = dev_df.label.values.astype(\"int32\")\n",
        "\n",
        "    return train_df, dev_df, train_labels, dev_labels\n",
        "\n",
        "train_df, dev_df, train_labels, dev_labels = load_and_preprocess_data(train_path,dev_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l6-Lkehnj1MY"
      },
      "source": [
        "Extract RoBERTa Embeddings\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c7zeVx8hj9SJ",
        "outputId": "677e201c-78c0-466a-a11d-e716eb44aa73"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFRobertaModel: ['lm_head.layer_norm.weight', 'lm_head.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.dense.bias', 'roberta.embeddings.position_ids', 'lm_head.bias']\n",
            "- This IS expected if you are initializing TFRobertaModel from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing TFRobertaModel from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights or buffers of the TF 2.0 model TFRobertaModel were not initialized from the PyTorch model and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        }
      ],
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
        "transformer = TFAutoModel.from_pretrained(MODEL_NAME)\n",
        "transformer.trainable = False\n",
        "\n",
        "def encode(df, max_len=50):\n",
        "    return tokenizer(\n",
        "        df['premise'].tolist(),\n",
        "        df['hypothesis'].tolist(),\n",
        "        padding=\"max_length\", truncation=True, max_length=max_len,\n",
        "        return_tensors=\"tf\"\n",
        "    )\n",
        "\n",
        "def compute_embeddings(input_ids, attention_mask, batch_size=256):\n",
        "    embeddings = []\n",
        "    dataset = tf.data.Dataset.from_tensor_slices((input_ids, attention_mask)).batch(batch_size)\n",
        "    for batch_ids, batch_mask in dataset:\n",
        "        output = transformer(batch_ids, attention_mask=batch_mask).last_hidden_state\n",
        "        embeddings.append(output.numpy())\n",
        "    return np.vstack(embeddings)\n",
        "\n",
        "train_encodings = encode(train_df)\n",
        "dev_encodings   = encode(dev_df)\n",
        "\n",
        "train_embeddings = compute_embeddings(train_encodings['input_ids'], train_encodings['attention_mask'])\n",
        "dev_embeddings   = compute_embeddings(dev_encodings['input_ids'], dev_encodings['attention_mask'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FKfjVuUckEFm"
      },
      "source": [
        "## Build BiMPM Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VBYB4KRiIvNH"
      },
      "source": [
        "### Custom BiMPM Matching Layer\n",
        "\n",
        "> Note:\n",
        "This function is used inside a Lambda layer in the model architecture to split the inputs into premise and hypothesis embeddings.\n",
        "Because the model was saved with this custom function, we need to **redefine it exactly as it was during training** so that Keras can correctly rebuild the model when loading it.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "ErJPXXK_kHf3"
      },
      "outputs": [],
      "source": [
        "\n",
        "class BiMPMMatching(Layer):\n",
        "    def __init__(self, hidden_size, num_perspectives, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self.hidden_size = hidden_size\n",
        "        self.num_perspectives = num_perspectives\n",
        "        self.W = self.add_weight(\n",
        "            shape=(num_perspectives, hidden_size * 2),\n",
        "            initializer=\"random_normal\",\n",
        "            trainable=True,\n",
        "            name=\"W_bimpm\"\n",
        "        )\n",
        "\n",
        "    def call(self, inputs):\n",
        "        u, v = inputs\n",
        "\n",
        "        def cosine_similarity(tensor_a, tensor_b):\n",
        "            a_expanded = tf.expand_dims(tensor_a, axis=2) * tf.reshape(self.W, (1, 1, self.num_perspectives, self.hidden_size * 2))\n",
        "            b_expanded = tf.expand_dims(tensor_b, axis=2) * tf.reshape(self.W, (1, 1, self.num_perspectives, self.hidden_size * 2))\n",
        "            return -tf.keras.losses.cosine_similarity(a_expanded, b_expanded, axis=-1)\n",
        "\n",
        "        def full_match(sequence, last_step_other_sequence):\n",
        "            last_step_other_sequence_expanded = tf.repeat(tf.expand_dims(last_step_other_sequence, 1), tf.shape(sequence)[1], axis=1)\n",
        "            return cosine_similarity(sequence, last_step_other_sequence_expanded)\n",
        "\n",
        "        def maxpool_match(sequence_a, sequence_b):\n",
        "            pooled_similarities = []\n",
        "            for i in range(sequence_a.shape[1]):\n",
        "                sequence_a_i = tf.repeat(tf.expand_dims(sequence_a[:, i, :], 1), sequence_b.shape[1], axis=1)\n",
        "                similarity_scores = cosine_similarity(sequence_a_i, sequence_b)\n",
        "                pooled_similarities.append(tf.reduce_max(similarity_scores, axis=1))\n",
        "            return tf.stack(pooled_similarities, axis=1)\n",
        "\n",
        "        full_match_premise = full_match(u, v[:, -1, :])\n",
        "        full_match_hypothesis = full_match(v, u[:, -1, :])\n",
        "        maxpool_premise = maxpool_match(u, v)\n",
        "        maxpool_hypothesis = maxpool_match(v, u)\n",
        "\n",
        "        return tf.concat([full_match_premise, full_match_hypothesis, maxpool_premise, maxpool_hypothesis], axis=-1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XTxi29PAM2aS"
      },
      "source": [
        "### Model Builder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "4QvAXq62M5Bd"
      },
      "outputs": [],
      "source": [
        "@register_keras_serializable()\n",
        "def split_premise_and_hypothesis(x):\n",
        "    return tf.split(x, num_or_size_splits=2, axis=1)\n",
        "\n",
        "\n",
        "def build_bimpm_model(hidden_size=128, num_perspectives=20, dropout_rate=0.4, learning_rate=3e-4):\n",
        "    model_input = Input(shape=(50, 768))\n",
        "    premise, hypothesis = Lambda(split_premise_and_hypothesis, name=\"split_input\")(model_input)\n",
        "\n",
        "    encode = Bidirectional(LSTM(hidden_size, return_sequences=True))\n",
        "    premise_encoded = encode(premise)\n",
        "    hypothesis_encoded = encode(hypothesis)\n",
        "\n",
        "    matching_layer = BiMPMMatching(hidden_size, num_perspectives)\n",
        "    matching_output = matching_layer([premise_encoded, hypothesis_encoded])\n",
        "\n",
        "    aggregation = Bidirectional(LSTM(hidden_size, return_sequences=True))(matching_output)\n",
        "    average_pooling = GlobalAveragePooling1D()(aggregation)\n",
        "    max_pooling = GlobalMaxPooling1D()(aggregation)\n",
        "    pooled = Concatenate()([average_pooling, max_pooling])\n",
        "\n",
        "    features = Dropout(dropout_rate)(pooled)\n",
        "    features = Dense(hidden_size, activation='relu')(features)\n",
        "    features = Dropout(dropout_rate)(features)\n",
        "\n",
        "    output = Dense(2, activation='softmax')(features)\n",
        "\n",
        "    model = Model(inputs=model_input, outputs=output)\n",
        "    model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate),\n",
        "                  loss=\"sparse_categorical_crossentropy\",\n",
        "                  metrics=[\"accuracy\"])\n",
        "    return model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sxZIxGxckYrL"
      },
      "source": [
        "Hyperparameter Tuning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "id": "5M8kjYjfkZxA"
      },
      "outputs": [],
      "source": [
        "def optimize_bimpm(trial):\n",
        "    # Hyperparameters to optimize\n",
        "    hidden_size = trial.suggest_categorical(\"hidden_size\", [64, 128])\n",
        "    num_perspectives = trial.suggest_categorical(\"num_perspectives\", [10, 20])\n",
        "    dropout_rate = trial.suggest_float(\"dropout_rate\", 0.2, 0.5)\n",
        "    learning_rate = trial.suggest_float(\"learning_rate\", 1e-5, 5e-4, log=True)\n",
        "    batch_size = trial.suggest_categorical(\"batch_size\", [16, 32])\n",
        "\n",
        "    model = build_bimpm_model(hidden_size, num_perspectives, dropout_rate, learning_rate)\n",
        "\n",
        "\n",
        "    early_stop = tf.keras.callbacks.EarlyStopping(monitor=\"val_loss\", patience=2, restore_best_weights=True)\n",
        "\n",
        "    history = model.fit(\n",
        "        train_embeddings, train_labels,\n",
        "        validation_data=(dev_embeddings, dev_labels),\n",
        "        epochs=10,\n",
        "        batch_size=batch_size,\n",
        "        callbacks=[early_stop],\n",
        "        verbose=0\n",
        "    )\n",
        "\n",
        "    # Get the best validation accuracy\n",
        "    best_validation_accuracy = max(history.history[\"val_accuracy\"])\n",
        "\n",
        "    # free memory between trials\n",
        "    tf.keras.backend.clear_session()\n",
        "    gc.collect()\n",
        "\n",
        "    return best_validation_accuracy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ptmWR0lbkhB1"
      },
      "source": [
        "Train and Save Best Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "su9XvcNrkiyh",
        "outputId": "cba4b0a7-0dcc-4b66-9a9a-e9bf0253bafe"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Found existing model. Loading...\n"
          ]
        }
      ],
      "source": [
        "if os.path.exists(model_path):\n",
        "    print(\"Found existing model. Loading...\")\n",
        "\n",
        "    model = tf.keras.models.load_model(\n",
        "        model_path,\n",
        "        custom_objects={\n",
        "            'BiMPMMatching': BiMPMMatching,\n",
        "            'split_premise_and_hypothesis': split_premise_and_hypothesis\n",
        "        }\n",
        "    )\n",
        "else:\n",
        "    print(\"No saved model found. Running Optuna and training a new model...\")\n",
        "\n",
        "    study = optuna.create_study(direction=\"maximize\")\n",
        "    study.optimize(optimize_bimpm, n_trials=15)\n",
        "\n",
        "    print(\"Best Hyperparameters:\", study.best_params)\n",
        "\n",
        "    best_hyperparameters = study.best_params\n",
        "    model = build_bimpm_model(\n",
        "        hidden_size=best_hyperparameters[\"hidden_size\"],\n",
        "        num_perspectives=best_hyperparameters[\"num_perspectives\"],\n",
        "        dropout_rate=best_hyperparameters[\"dropout_rate\"],\n",
        "        learning_rate=best_hyperparameters[\"learning_rate\"]\n",
        "    )\n",
        "\n",
        "    early_stop = tf.keras.callbacks.EarlyStopping(monitor=\"val_loss\", patience=3, restore_best_weights=True)\n",
        "    model.fit(train_embeddings, train_labels, validation_data=(dev_embeddings, dev_labels),\n",
        "              epochs=15, batch_size=best_hyperparameters[\"batch_size\"], callbacks=[early_stop])\n",
        "    model.save(model_path)\n",
        "    print(\"Model trained and saved to:\", model_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aO7-6scGkzMo"
      },
      "source": [
        " Evaluation on Dev Set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RnO2RVg4k3Cq",
        "outputId": "23ba7cb9-23e8-4f17-f2f7-5e2c1484867d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m211/211\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 38ms/step\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.7466    0.7397    0.7431      3258\n",
            "           1     0.7583    0.7648    0.7615      3478\n",
            "\n",
            "    accuracy                         0.7527      6736\n",
            "   macro avg     0.7524    0.7523    0.7523      6736\n",
            "weighted avg     0.7526    0.7527    0.7526      6736\n",
            "\n"
          ]
        }
      ],
      "source": [
        "preds = model.predict(dev_embeddings).argmax(axis=1)\n",
        "print(classification_report(dev_labels, preds, digits=4))"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
