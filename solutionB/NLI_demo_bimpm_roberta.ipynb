{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n8HFhJwiKshh"
      },
      "source": [
        "### **Solution B:** Deep learning-based approaches that do not employ transformer architectures   \n",
        "Our final model uses a BiMPM-inspired architecture with frozen RoBERTa embeddings. The model captures matching perspectives between the encoded premise and hypothesis via a custom multi-perspective matching layer. Pre-trained RoBERTa embeddings are computed once and used as static inputs. The model was optimized using Optuna, and trained on the full dataset.\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "#### **Expected Input**\n",
        "- CSV file named `test.csv`\n",
        "- Must contain two columns: `premise` and `hypothesis`  \n",
        "- No labels required\n",
        "\n",
        "#### **Expected Output**\n",
        "- `group_33_B.csv` — one prediction per line (0 or 1)\n",
        "\n",
        "---\n",
        "\n",
        "#### **Instructions to Run**\n",
        "1. Place your test file in `Data/test.csv`\n",
        "2. Ensure the trained model is available at:  \n",
        "   `savedModel/best_bimpm_model.keras`  \n",
        "3. Run all cells in order to generate predictions\n",
        "\n",
        "> **Note**: - The model uses precomputed RoBERTa embeddings (frozen) and a BiMPM-style architecture with multi-perspective matching.\n",
        "\n",
        "> **Note**: This notebook is also used to predict on the `test.csv`, the predictions will be saved in `../predictions/group_33_b_csv`\n",
        " \n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A3tA4yBMFchv"
      },
      "source": [
        "Setup & Install Packages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "55p9809cFJVQ",
        "outputId": "f7ab2bf2-666c-4b57-8f5d-d05f6480e8eb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive')\n",
        "\n",
        "!pip install transformers tensorflow --quiet\n",
        "import re, string\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "from transformers import AutoTokenizer, TFAutoModel\n",
        "from tensorflow.keras.layers import Layer\n",
        "from keras.saving import register_keras_serializable\n",
        "\n",
        "# test_path = \"/content/drive/MyDrive/dataset/training_data/NLI/test.csv\"\n",
        "# model_path = \"/content/drive/MyDrive/dataset/training_data/NLI/best_bimpm_model.keras\"\n",
        "# test_predictions = \"/content/drive/MyDrive/dataset/training_data/NLI/Group_33_B.csv\"\n",
        "\n",
        "model_path = \"savedModels/best_bimpm_model.keras\"\n",
        "test_path = \"../Data/test.csv\"\n",
        "test_predictions = \"../predictions/Group_33_B.csv\"\n",
        "\n",
        "MODEL_NAME = \"roberta-base\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JHIXL2o2Fqs5"
      },
      "source": [
        "Load and Preprocess Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "jF4I8aq7Frst"
      },
      "outputs": [],
      "source": [
        "def clean_text(text):\n",
        "    text = ''.join(ch for ch in text if ch not in string.punctuation)\n",
        "    text = re.sub(r'\\s+', ' ', text).strip()\n",
        "    return text\n",
        "\n",
        "test_df = pd.read_csv(test_path)\n",
        "test_df['premise'] = test_df['premise'].fillna(\"\").apply(clean_text)\n",
        "test_df['hypothesis'] = test_df['hypothesis'].fillna(\"\").apply(clean_text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mw28D8VsFuZ5"
      },
      "source": [
        "Extract RoBERTa Embeddings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S7ShiplBFw0r",
        "outputId": "3676b979-70f4-431a-f118-2259e3702da1"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFRobertaModel: ['lm_head.dense.bias', 'lm_head.layer_norm.bias', 'roberta.embeddings.position_ids', 'lm_head.layer_norm.weight', 'lm_head.dense.weight', 'lm_head.bias']\n",
            "- This IS expected if you are initializing TFRobertaModel from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing TFRobertaModel from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights or buffers of the TF 2.0 model TFRobertaModel were not initialized from the PyTorch model and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        }
      ],
      "source": [
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
        "transformer = TFAutoModel.from_pretrained(MODEL_NAME)\n",
        "transformer.trainable = False\n",
        "\n",
        "def encode(df, max_len=50):\n",
        "    return tokenizer(\n",
        "        df['premise'].tolist(),\n",
        "        df['hypothesis'].tolist(),\n",
        "        padding=\"max_length\", truncation=True, max_length=max_len,\n",
        "        return_tensors=\"tf\"\n",
        "    )\n",
        "\n",
        "def compute_embeddings(input_ids, attention_mask, batch_size=256):\n",
        "    embeddings = []\n",
        "    dataset = tf.data.Dataset.from_tensor_slices((input_ids, attention_mask)).batch(batch_size)\n",
        "    for batch_ids, batch_mask in dataset:\n",
        "        output = transformer(batch_ids, attention_mask=batch_mask).last_hidden_state\n",
        "        embeddings.append(output.numpy())\n",
        "    return np.vstack(embeddings)\n",
        "\n",
        "test_encodings = encode(test_df)\n",
        "\n",
        "test_embeddings = compute_embeddings(test_encodings['input_ids'], test_encodings['attention_mask'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U2Y-HQ7-F8V6"
      },
      "source": [
        "### Custom BiMPM Matching Layer\n",
        "\n",
        "> Note:\n",
        "This function is used inside a Lambda layer in the model architecture to split the inputs into premise and hypothesis embeddings.\n",
        "Because the model was saved with this custom function, we need to **redefine it exactly as it was during training** so that Keras can correctly rebuild the model when loading it.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "WR9R4xuiF35v"
      },
      "outputs": [],
      "source": [
        "class BiMPMMatching(Layer):\n",
        "    def __init__(self, hidden_size, num_perspectives, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self.hidden_size = hidden_size\n",
        "        self.num_perspectives = num_perspectives\n",
        "        self.W = self.add_weight(\n",
        "            shape=(num_perspectives, hidden_size * 2),\n",
        "            initializer=\"random_normal\",\n",
        "            trainable=True,\n",
        "            name=\"W_bimpm\"\n",
        "        )\n",
        "\n",
        "    def call(self, inputs):\n",
        "        premise_encoded, hypothesis_encoded = inputs\n",
        "\n",
        "        def cosine_similarity(tensor_a, tensor_b):\n",
        "            a_expanded = tf.expand_dims(tensor_a, axis=2) * tf.reshape(self.W, (1, 1, self.num_perspectives, self.hidden_size * 2))\n",
        "            b_expanded = tf.expand_dims(tensor_b, axis=2) * tf.reshape(self.W, (1, 1, self.num_perspectives, self.hidden_size * 2))\n",
        "            return -tf.keras.losses.cosine_similarity(a_expanded, b_expanded, axis=-1)\n",
        "\n",
        "        def full_match(sequence, last_step_other_sequence):\n",
        "            last_step_other_sequence_expanded = tf.repeat(tf.expand_dims(last_step_other_sequence, 1), tf.shape(sequence)[1], axis=1)\n",
        "            return cosine_similarity(sequence, last_step_other_sequence_expanded)\n",
        "\n",
        "        def maxpool_match(sequence_a, sequence_b):\n",
        "            pooled_similarities = []\n",
        "            for i in range(sequence_a.shape[1]):\n",
        "                sequence_a_i = tf.repeat(tf.expand_dims(sequence_a[:, i, :], 1), tf.shape(sequence_b)[1], axis=1)\n",
        "                similarity_scores = cosine_similarity(sequence_a_i, sequence_b)\n",
        "                pooled_similarities.append(tf.reduce_max(similarity_scores, axis=1))\n",
        "            return tf.stack(pooled_similarities, axis=1)\n",
        "\n",
        "        full_match_premise = full_match(premise_encoded, hypothesis_encoded[:, -1, :])\n",
        "        full_match_hypothesis = full_match(hypothesis_encoded, premise_encoded[:, -1, :])\n",
        "        maxpool_premise = maxpool_match(premise_encoded, hypothesis_encoded)\n",
        "        maxpool_hypothesis = maxpool_match(hypothesis_encoded, premise_encoded)\n",
        "\n",
        "        return tf.concat([full_match_premise, full_match_hypothesis, maxpool_premise, maxpool_hypothesis], axis=-1)\n",
        "\n",
        "@register_keras_serializable()\n",
        "def split_premise_and_hypothesis(x):\n",
        "    return tf.split(x, num_or_size_splits=2, axis=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IK4_wCK2GFNK"
      },
      "source": [
        "Load Model and Predict"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0Bjed9DIGE71",
        "outputId": "69626113-b9ef-4b43-81c5-a13ec0bd363f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading model from: /content/drive/MyDrive/dataset/training_data/NLI/best_bimpm_model.keras\n",
            "Predicting...\n",
            "\u001b[1m104/104\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 55ms/step\n"
          ]
        }
      ],
      "source": [
        "print(\"Loading model from:\", model_path)\n",
        "\n",
        "model = tf.keras.models.load_model(model_path, custom_objects={\n",
        "    'BiMPMMatching': BiMPMMatching,\n",
        "    'split_premise_and_hypothesis': split_premise_and_hypothesis\n",
        "})\n",
        "\n",
        "print(\"Predicting...\")\n",
        "predictions = model.predict(test_embeddings).argmax(axis=1)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sIu7MvHMGI1a"
      },
      "source": [
        "Save Test Predictions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MPcdchrvGQQP",
        "outputId": "fb675be4-d4a2-4743-cad6-282d3d7d6004"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saved predictions to: /content/drive/MyDrive/dataset/test_data/NLI/group_33_B.csv\n"
          ]
        }
      ],
      "source": [
        "pd.DataFrame({'prediction': predictions}).to_csv(test_predictions, index=False)\n",
        "\n",
        "print(\"Saved predictions to:\", test_predictions)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
